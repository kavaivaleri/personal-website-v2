[
  {
    "id": "what-is-jepa",
    "title": "What is Joint Embedding Predictive Architecture (JEPA)?",
    "description": "Technical deep dive on JEPA (Joint-Embedding Predictive Architecture), a framework for self-supervised learning and world models proposed by Yann LeCun. Received public endorsement from Yann LeCun, its creator and one of the godfathers of AI, who called it an “Excellent blog post” on both LinkedIn and Twitter/X.",
    "url": "https://www.turingpost.com/p/jepa",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2024-06-13",
=======
    "publishedAt": "2024-06-13T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "10",
    "readTime": "12 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2024-06-13"
=======
    "createdAt": "2024-06-13T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "what-is-retrieval-augmented-generation-rag",
    "title": "What is Retrieval-Augmented Generation (RAG)?",
    "description": "Primer on Retrieval-Augmented Generation (RAG), explaining what it is, which core LLM limitations it addresses, how its architecture works, and why it has become so popular.",
    "url": "https://www.turingpost.com/p/rag",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-10-05",
=======
    "publishedAt": "2023-10-05T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "20",
    "readTime": "13 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-10-05"
=======
    "createdAt": "2023-10-05T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "transformer-and-diffusion-based-foundation-models",
    "title": "Transformer and Diffusion-Based Foundation Models",
    "description": "Authored the technical sections on Transformer-based and Diffusion-based models and conducted research on generative model architectures to trace the origins of diffusion models; other generative models aren't covered due to space constraints.",
    "url": "https://www.turingpost.com/p/transformerdiffusion",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-10-26",
=======
    "publishedAt": "2023-10-26T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "21",
    "readTime": "14 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-10-26"
=======
    "createdAt": "2023-10-26T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "what-is-mixture-of-experts-moe",
    "title": "What is Mixture-of-Experts (MoE)?",
    "description": "Introduction to Mixture-of-Experts (MoE) models, tracing their history and original architecture, explaining why modular “experts” with conditional computation can outperform a single dense network, and showing how MoEs combined with Transformers underpin modern scalable systems like Mistral, DBRX, Jamba, Grok-1, and Arctic.",
    "url": "https://www.turingpost.com/p/moe",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2024-05-25",
=======
    "publishedAt": "2024-05-25T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "30",
    "readTime": "14 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2024-05-25"
=======
    "createdAt": "2024-05-25T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "graphrag",
    "title": "What is Graph RAG approach?",
    "description": "Guide to Graph RAG, explaining how graph-structured retrieval upgrades classic RAG by addressing its key limitations and leveraging knowledge graphs for better reasoning over complex information.",
    "url": "https://www.turingpost.com/p/graphrag",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2024-06-06",
=======
    "publishedAt": "2024-06-06T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "31",
    "readTime": "15 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2024-06-06"
=======
    "createdAt": "2024-06-06T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "what-is-longrag-framework",
    "title": "What is LongRAG framework?",
    "description": "Explainer on the LongRAG framework, outlining the limitations of standard long-context RAG and introducing LongRAG's architecture and intuition. How it restructures retrieval to better handle long documents—along with its key advantages and curated resources for further study.",
    "url": "https://www.turingpost.com/p/longrag",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2024-06-06",
=======
    "publishedAt": "2024-06-06T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "32",
    "readTime": "10 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2024-06-06"
=======
    "createdAt": "2024-06-06T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "what-is-ai-red-teaming",
    "title": "What is AI Red Teaming?",
    "description": "Overview of AI red teaming, its history and core concepts, how it differs from traditional security testing, the key components of a red team exercise, common vulnerabilities uncovered in practice, and the unique challenges faced by practitioners testing modern AI systems.",
    "url": "https://learnprompting.org/blog/what-is-ai-red-teaming",
    "publication": "Learn Prompting",
    "category": "Guide, AI Safety",
<<<<<<< HEAD
    "publishedAt": "2025-09-03",
=======
    "publishedAt": "2025-09-03T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "40",
    "readTime": "1 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2025-09-03"
=======
    "createdAt": "2025-09-03T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "understanding-fun-tuning-how-researchers-found-a-new-way-to-hack-ai-models",
    "title": "Understanding Fun-Tuning: How Researchers Found a New Way to Hack AI Models",
    "description": "Investigation of a new “fun-tuning” attack on Google's Gemini models, showing how researchers used loss signals from the fine-tuning API to systematically bypass safety rules with increasingly effective prompt-injection attacks.",
    "url": "https://learnprompting.org/blog/fun-tuning-prompt-hacking-gemini-by-exploiting-gemini-free-api",
    "publication": "Learn Prompting",
    "category": "Guide, AI Safety",
<<<<<<< HEAD
    "publishedAt": "2025-09-03",
=======
    "publishedAt": "2025-09-03T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "41",
    "readTime": "1 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2025-09-03"
=======
    "createdAt": "2025-09-03T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "hacking-chatgpts-memory-system-using-prompt-injection",
    "title": "Hacking ChatGPT's Memory System using Prompt Injection",
    "description": "Technical analysis of ChatGPT's memory feature and its vulnerabilities to prompt injection attacks based on a recent real-world attack scenario.",
    "url": "https://learnprompting.org/blog/prompt-injection-hacking-chatgpt-memories",
    "publication": "Learn Prompting",
    "category": "Guide, AI Safety",
<<<<<<< HEAD
    "publishedAt": "2025-09-03",
=======
    "publishedAt": "2025-09-03T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "42",
    "readTime": "1 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2025-09-03"
=======
    "createdAt": "2025-09-03T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "there-would-have-been-no-llms-without-this-history-of-llms-3",
    "title": "There Would Have Been No LLMs Without This",
    "description": "Historical deep dive into the AI and ML breakthroughs of the 1990s to mid-2000s, showing how foundational ideas from that era, developed under severe computational constraints, paved the way for today's large language models.",
    "url": "https://www.turingpost.com/p/llmshistory3",
    "publication": "Turing Post",
    "category": "History",
<<<<<<< HEAD
    "publishedAt": "2023-07-07",
=======
    "publishedAt": "2023-07-07T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "50",
    "readTime": "15 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-07-07"
=======
    "createdAt": "2023-07-07T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "a-deep-dive-into-llama",
    "title": "A Deep Dive Into LLaMA, Falcon, Llama 2",
    "description": "Deep dive into three leading open-source LLMs according to Hugging Face's Open LLM Leaderboard: LLaMA, LLaMa 2 and Falcon, and their top fine-tuned variants",
    "url": "https://www.turingpost.com/p/top3llmsope",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-07-28",
=======
    "publishedAt": "2023-07-28T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "51",
    "readTime": "15 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-07-28"
=======
    "createdAt": "2023-07-28T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "how-to-leverage-open-source-llms-in-your-project",
    "title": "How to Leverage Open-Source LLMs in Your Project",
    "description": "Gathered expert commentary on implementing open-source LLMs in production, featuring insights from five industry experts: Edward Beeching (Hugging Face research scientist and co-creator of Open LLMs leaderboard), Rajiv Shah (Hugging Face ML engineer), Aniket Maurya (Lightning AI developer advocate), Lianmin Zheng (UC Berkeley PhD student and Vicuna contributor), and Devis Lucato (Microsoft principal architect, Semantic Kernel).",
    "url": "https://www.turingpost.com/p/practicalllms",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-08-04",
=======
    "publishedAt": "2023-08-04T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "52",
    "readTime": "13 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-08-04"
=======
    "createdAt": "2023-08-04T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "from-chain-of-thoughts-to-skeleton-of-thoughts",
    "title": "From Chain-of-Thoughts to Skeleton-of-Thoughts, and everything in between",
    "description": "Survey of the Chain-of-Thought Prompting lineage, tracing how the original NeurIPS 2022 Google Brain paper sparked a wave of follow-up work and “chain”-style methods, from Self-Consistency, Zero-Shot CoT, Auto-CoT, and Program-of-Thoughts to Multimodal CoT, Tree-of-Thoughts, Graph-of-Thoughts, Algorithm-of-Thoughts, and Skeleton-of-Thought, and mapping their core ideas, innovations, and relationships in a single, up-to-date reference.",
    "url": "https://www.turingpost.com/p/cot",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-10-18",
=======
    "publishedAt": "2023-10-18T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "60",
    "readTime": "12 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-10-18"
=======
    "createdAt": "2023-10-18T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "prompt-engineering-vs-fine-tuning-how-to-choose-the-right-approach-for-your-needs",
    "title": "Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs",
    "description": "Co-authored a guide comparing prompt engineering and fine-tuning as effective techniques for adapting LLMs to specific tasks.",
    "url": "https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning",
    "publication": "Learn Prompting",
    "category": "Guide, Prompt Engineering",
<<<<<<< HEAD
    "publishedAt": "2025-09-03",
=======
    "publishedAt": "2025-09-03T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "61",
    "readTime": "1 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2025-09-03"
=======
    "createdAt": "2025-09-03T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "llm-parameters-explained-a-practical-guide-with-examples-for-openai-api-in-python",
    "title": "LLM Parameters Explained: A Practical Guide with Examples for OpenAI API in Python",
    "description": "Edited a practical guide to the key parameters that control LLM behavior: temperature, top-p, max tokens, frequency and presence penalties, and stop sequences, explaining how each one affects randomness, coherence, length, repetition, and novelty, with concrete examples using OpenAI's ChatGPT and API so you can reliably steer outputs from deterministic to creative.",
    "url": "https://learnprompting.org/blog/llm-parameters",
    "publication": "Learn Prompting",
    "category": "Guide, Prompt Engineering",
<<<<<<< HEAD
    "publishedAt": "2025-09-03",
=======
    "publishedAt": "2025-09-03T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "62",
    "readTime": "1 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2025-09-03"
=======
    "createdAt": "2025-09-03T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "what-is-low-rank-adaptation-lora",
    "title": "What is Low-Rank Adaptation (LoRA)?",
    "description": "Guide to Low-Rank Adaptation (LoRA) in the context of large language models, explaining when full fine-tuning is truly necessary, how LoRA compares to other adaptation techniques, the core intuition behind it, how it works under the hood, and why it offers a more efficient way to specialize billion-parameter LLMs.",
    "url": "https://www.turingpost.com/p/lora",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-11-30",
=======
    "publishedAt": "2023-11-30T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "70",
    "readTime": "9 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-11-30"
=======
    "createdAt": "2023-11-30T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "what-are-hallucinations",
    "title": "What are Hallucinations: a Critical Challenge or an Opportunity?",
    "description": "Explainer on hallucinations in foundation models - what they are, why they occur, how to detect them, when they're harmful or occasionally useful. Practical strategies to reduce or mitigate them, and a curated set of tools, libraries, and datasets for working with hallucinations in practice.",
    "url": "https://www.turingpost.com/p/hallucination",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2024-01-04",
=======
    "publishedAt": "2024-01-04T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "71",
    "readTime": "18 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2024-01-04"
=======
    "createdAt": "2024-01-04T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "how-to-handle-missing-values",
    "title": "How to Handle Missing Values?",
    "description": "Gathered expert commentary on handling missing values in machine learning datasets, featuring insights from three industry experts: Ryan Kearns (Founding Data Scientist, Monte Carlo), David Berenstein (Developer Advocate, Argilla), and Abhishek Pawar (Senior Data Scientist, Precisely).",
    "url": "https://www.turingpost.com/p/missingvalues",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-09-07",
=======
    "publishedAt": "2023-09-07T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "72",
    "readTime": "9 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-09-07"
=======
    "createdAt": "2023-09-07T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "algorithm-or-personality-the-significance-of-anthropomorphism-in-ai",
    "title": "Algorithm or Personality? The Significance of Anthropomorphism in AI",
    "description": "Exploration of the origins of anthropomorphism in AI, examining why humans instinctively attribute human qualities to non-human systems, how fictional narratives and media portrayals of robots and AI shape public perception, and the psychological implications of these tendencies for human–AI relationships and trust.",
    "url": "https://www.turingpost.com/p/anthropomorphisminai",
    "publication": "Turing Post",
    "category": "Technical",
<<<<<<< HEAD
    "publishedAt": "2023-05-31",
=======
    "publishedAt": "2023-05-31T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "80",
    "readTime": "12 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2023-05-31"
=======
    "createdAt": "2023-05-31T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "gpt-4o-image-generation-a-complete-guide-12-prompt-examples",
    "title": "GPT-4o Image Generation: A Complete Guide + 12 Prompt Examples",
    "description": "A comprehensive guide to GPT-4o Image Generation with 12 detailed prompt examples to help readers create the exact image they envision",
    "url": "https://learnprompting.org/blog/guide-openai-4o-image-generation",
    "publication": "Learn Prompting",
    "category": "Guide, AI Tools",
<<<<<<< HEAD
    "publishedAt": "2025-09-03",
=======
    "publishedAt": "2025-09-03T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "81",
    "readTime": "1 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2025-09-03"
=======
    "createdAt": "2025-09-03T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  },
  {
    "id": "the-mysterious-ai-reading-list-ilya-sutskevers-recommendations",
    "title": "The Mysterious AI Reading List: Ilya Sutskever's Recommendations",
    "description": "A piece on the elusive AI reading list Ilya Sutskever reportedly gave John Carmack, tracing the origin of this “90% of what matters today” claim and how the unseen ~40-paper list became a legend in the ML community.",
    "url": "https://www.turingpost.com/p/ilya-sutskever-reading-list",
    "publication": "Turing Post",
    "category": "Listicle",
<<<<<<< HEAD
    "publishedAt": "2024-05-19",
=======
    "publishedAt": "2024-05-19T00:00:00.000Z",
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
    "featured": "true",
    "priority": "82",
    "readTime": "9 min read",
    "imageUrl": null,
<<<<<<< HEAD
    "createdAt": "2024-05-19"
=======
    "createdAt": "2024-05-19T00:00:00.000Z"
>>>>>>> d547dbf3fbe49807e383cc64880737bf58d57073
  }
]