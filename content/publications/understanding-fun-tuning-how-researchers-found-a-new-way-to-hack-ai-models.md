---
title: "Understanding Fun-Tuning: How Researchers Found a New Way to Hack AI Models"
description: "Investigation of a new “fun-tuning” attack on Google's Gemini models, showing how researchers used loss signals from the fine-tuning API to systematically bypass safety rules with increasingly effective prompt-injection attacks."
url: "https://learnprompting.org/blog/fun-tuning-prompt-hacking-gemini-by-exploiting-gemini-free-api"
publication: "Learn Prompting"
category: "Guide, AI Safety"
publishedAt: "2025-09-03"
featured: true
priority: "41"
readTime: "1 min read"
---

An explanation of how researchers discovered a systematic way to make AI models ignore their safety guardrails by exploiting fine-tuning APIs