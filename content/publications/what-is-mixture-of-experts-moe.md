---
title: "What is Mixture-of-Experts (MoE)?"
description: "Introduction to Mixture-of-Experts (MoE) models, tracing their history and original architecture, explaining why modular “experts” with conditional computation can outperform a single dense network, and showing how MoEs combined with Transformers underpin modern scalable systems like Mistral, DBRX, Jamba, Grok-1, and Arctic."
url: "https://www.turingpost.com/p/moe"
publication: "Turing Post"
category: "Technical"
publishedAt: "2024-05-25"
featured: true
priority: "30"
readTime: "14 min read"
---
